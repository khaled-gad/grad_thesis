% !TEX root = ../Thesis.tex
\section{Introduction}

\subsection{Motivation}

The rapid evolution of artificial intelligence (AI) and digital signal processing (DSP) applications has fundamentally transformed the computational requirements of modern computing systems. AI workloads, particularly deep learning models, demand massive parallel computation for operations such as matrix multiplication, convolution, and tensor operations. Similarly, DSP applications require intensive mathematical operations including filtering, Fourier transforms, and correlation analysis. These computational patterns share a common characteristic: they involve highly parallel, data-intensive operations that can benefit significantly from vectorized execution.

% FLOPs growth figure here
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/Flops.png}
	\caption[Growth in AI model computational requirements]{Exponential growth in AI model computational requirements over time, showing the dramatic increase in FLOPs required for training state-of-the-art models.\cite{ourworldindata}}
    \label{fig:ai-growth}
\end{figure}

Traditional scalar processors, designed primarily for sequential instruction execution, face significant challenges when processing these data-parallel workloads. The von Neumann architecture, with its single instruction stream operating on individual data elements, creates a fundamental bottleneck for AI and DSP applications. For example, a typical convolution operation in a convolutional neural network (CNN) involves millions of multiply-accumulate operations that could theoretically be executed in parallel, but scalar processors must execute them sequentially, leading to substantial performance degradation.

The performance gap becomes even more pronounced when considering the memory bandwidth requirements of AI and DSP applications. These workloads often involve large datasets that exceed the capacity of processor caches, leading to frequent memory accesses. The arithmetic intensity---the ratio of computation to memory access---of many AI and DSP kernels is relatively low, meaning that processors spend significant time waiting for data rather than performing useful computation. This phenomenon, commonly referred to as the ``memory wall,'' represents a fundamental challenge for data-intensive computing.

\subsection{Limitations of Current Solutions}

Current solutions to these challenges have primarily relied on specialized hardware architectures and proprietary vector processing extensions. Graphics Processing Units (GPUs) have become the de facto standard for AI acceleration due to their thousands of parallel cores designed for data-parallel computation. However, GPUs present several limitations for AI and DSP applications:

\begin{itemize}
    \item \textbf{Power consumption}: GPUs consume significant power, making them unsuitable for edge computing and mobile applications where energy efficiency is paramount.
    \item \textbf{Programming complexity}: The GPU programming model, while powerful, requires specialized knowledge (Compute Unified Device Architecture (CUDA), Open Computing Language (OpenCL)) and often results in complex code that is difficult to optimize and maintain.
    \item \textbf{Integration challenges}: GPUs are discrete components requiring separate memory spaces and Peripheral Component Interconnect Express (PCIe) communication, introducing latency and bandwidth constraints for certain workloads.
\end{itemize}

Proprietary vector processing solutions, such as Intel's Advanced Vector Extensions (AVX) and ARM's NEON, provide another approach to accelerating data-parallel workloads. These extensions add vector processing capabilities to traditional CPU architectures, allowing multiple data elements to be processed with a single instruction (SIMD). However, these solutions have significant drawbacks that limit their effectiveness and adoption:

\begin{enumerate}
    \item \textbf{Vendor lock-in}: Proprietary vector extensions create situations where software optimized for one vendor's vector instructions cannot efficiently run on competitors' hardware, fragmenting the software ecosystem.
    
    \item \textbf{Fixed vector widths}: These extensions typically use fixed vector widths (e.g., 128-bit for NEON, 256-bit or 512-bit for AVX), meaning that software must be written for specific vector lengths and may not efficiently utilize processors with different vector capabilities.
    
    \item \textbf{Licensing costs}: Licensing costs and restrictions associated with proprietary architectures can be prohibitive, particularly for smaller companies and research institutions developing specialized AI and DSP applications.
    
    \item \textbf{Limited extensibility}: The closed nature of proprietary Instruction Set Architectures (ISAs) makes it difficult for researchers and developers to experiment with custom instructions or architectural modifications.
\end{enumerate}

\subsection{The RISC-V Vector Extension as a Solution}

RISC-V Vector Extensions (RVV) emerged as a promising solution to address these challenges by providing an open-source, royalty-free vector processing architecture specifically designed for data-parallel computation \cite{riscv-v-spec}. Unlike proprietary alternatives, RVV is developed through an open, collaborative process that ensures the architecture meets the diverse needs of the computing community.

The most distinctive feature of RVV is its \textbf{vector-length agnostic (VLA)} programming model, which represents a fundamental departure from traditional fixed-width Single Instruction, Multiple Data (SIMD) approaches. In conventional vector processing, software must be written for specific vector widths, and different code paths are often required to support processors with different vector capabilities. RVV's vector-length agnostic model allows the same code to run efficiently across processors with different vector lengths, from embedded systems with short vectors to high-performance computing systems with very long vectors.

This flexibility is particularly valuable for AI and DSP applications, which span a wide range of computing environments with different performance and power requirements. An AI inference algorithm written using RVV can run efficiently on both a power-constrained edge device with 128-bit vectors and a high-performance server processor with 2048-bit vectors, without requiring code modifications or recompilation. The ratification of RVV Version 1.0 in late 2021 provided a crucial guarantee of stability, signaling to the industry that the architecture was mature and ready for widespread adoption.

\subsection{Problem Statement}

Despite the architectural advantages of the RISC-V Vector Extension, developing optimized kernels for RVV remains a challenging task. New developers often face two major obstacles:

\begin{enumerate}
    \item \textbf{Lack of standardized workflows}: There is currently no unified methodology for vector kernel development that encompasses design, verification, and performance evaluation in a cohesive framework.
    
    \item \textbf{Limited guidance on verification and evaluation}: While performance measurement is essential to demonstrate the benefits of vectorization, ensuring functional correctness is equally critical, especially when kernels are applied in sensitive domains such as artificial intelligence or embedded systems.
\end{enumerate}

Furthermore, the absence of widely available RVV-capable silicon necessitates reliance on emulation and Register Transfer Level (RTL) simulation environments for development and validation. This creates additional complexity in establishing reproducible benchmarking methodologies that can provide meaningful performance insights.

\subsection{Project Objectives}

This thesis investigates the role and potential impact of RISC-V Vector Extensions in accelerating AI and DSP applications. The primary objectives of this project are:

\begin{enumerate}
    \item \textbf{Develop a systematic framework} for the design, verification, and performance evaluation of RISC-V vector kernels that integrates open-source tools into a reproducible workflow.
    
    \item \textbf{Implement a comprehensive library of optimized RVV kernels} categorized by their computational patterns:
    \begin{itemize}
        \item \textbf{Compute-Intensive Linear Operators:} Matrix multiplication (\textit{matmul}), fully connected layers (\textit{dense}), and both standard and transposed convolutions.
        \item \textbf{Pointwise Activation and Arithmetic:} Rectified Linear Unit (\textit{ReLU}), Leaky ReLU, bias addition, and element-wise tensor addition.
        \item \textbf{Statistical and Normalization Kernels:} Batch normalization and the Softmax probability function.
        \item \textbf{Spatial Reduction and Indexing:} Max pooling, as well as ONNX-style data movement operations including Gather, GatherElements, and ScatterElements.
        \item \textbf{Decision Kernels:} Post-processing operations such as Non-Maximum Suppression (NMS).
    \end{itemize}
    
    \item \textbf{Establish functional verification methodologies} using ONNX (Open Neural Network Exchange) \cite{onnx} as a golden reference framework, ensuring correctness through quantitative metrics such as Signal-to-Noise Ratio (SNR) and Maximum Absolute Error (MaxAbs).
    
    \item \textbf{Conduct cycle-accurate performance evaluation} using RTL simulation with the Vicuna \cite{vicuna} and Ara \cite{ara} vector coprocessors, quantifying the speedup achieved through vectorization over scalar implementations.
    
    \item \textbf{Analyze the trade-offs} between different architectural approaches (high-performance vs. embedded) and provide insights into optimal kernel design strategies for various deployment scenarios.
\end{enumerate}

\subsection{Project Contributions}

This thesis makes the following contributions to the field of RISC-V vector processing for machine learning and DSP applications:

\begin{enumerate}
    \item \textbf{A reproducible three-step framework} integrating kernel design using RVV C-intrinsics, functional verification against ONNX golden references, and cycle-accurate performance analysis using RTL simulation with Verilator. This framework provides a systematic methodology that can be adopted by other researchers and developers.
    
    \item \textbf{A library of optimized RVV kernels (RaiVeX Library)} implementing a wide array of operations for neural network inference and signal processing. The library demonstrates efficient use of advanced RVV features, including strip-mining loops, vector-length agnostic programming, Length Multiplier (LMUL) configuration, and masked operations.
    
    \item \textbf{Performance characterization on a high-performance RTL platform:} Implementation and evaluation were conducted using \textbf{Ara}, a 64-bit vector coprocessor targeting application-class workloads. The study explores the impact of Ara's microarchitecture, including its configurable lane counts and full floating-point support, on kernel efficiency.
    
    \item \textbf{Quantitative analysis} demonstrating significant performance gains through vectorization. Experimental results on the Ara co-processor show substantial speedups ranging from $4\times$ to over $70\times$ compared to scalar baseline implementations, highlighting the efficiency of the RVV ISA for data-parallel workloads.
    
    \item \textbf{High-level language integration through Python wrappers:} The development of a bridging layer using the \texttt{ctypes} foreign function interface, allowing the execution of optimized C++ vector kernels directly from a Python environment. This enables high-level algorithmic development and testing while maintaining the raw performance and hardware-level control of C++ implementations.
\end{enumerate}

\subsection{Thesis Organization}

The remainder of this thesis is organized as follows to guide the reader from foundational concepts to our specific architectural contributions and empirical results:

\textbf{Chapter 2: Background and Related Work} establishes the theoretical foundation of the RISC-V ISA and its modular extension system. It details the RISC-V Vector (RVV) Extension, focusing on Vector-Length Agnosticism (VLA) and the intrinsic-based programming model. Finally, it reviews the current state of RVV research, covering virtual prototyping, reliability analysis, and existing hardware accelerators.

\textbf{Chapter 3: Methodology: Architecture \& Implementations} presents the framework for kernel development and functional validation. It covers the toolchain setup, including the RISC-V GNU toolchain \cite{riscv-gnu-toolchain}, QEMU emulator \cite{qemu-riscv}, and ONNX framework \cite{onnx}. The chapter details the design of vectorized patterns for operations ranging from compute-bound FMAs to post-processing Non-Maximum Suppression (NMS), concluding with a comprehensive verification of these kernels integrated into full-scale LeNet-5 \cite{lecun1998gradient} and Tiny-YOLOv2 \cite{redmon2016yolo} inference pipelines.

\textbf{Chapter 4: Methodology: Performance Validation} shifts the focus to hardware-level evaluation using cycle-accurate RTL simulation with Verilator \cite{verilator}. It justifies the selection of the Ara vector coprocessor \cite{ara} over the Vicuna core \cite{vicuna} and describes the benchmarking environment. The chapter provides a deep dive into performance results, analyzing how arithmetic intensity, register grouping (LMUL), and memory bandwidth impact speedups across compute-bound, sliding-window, and pointwise kernels.

\textbf{Chapter 5: Open Source Library Architecture} describes the design and implementation of the RaiVeX Library software interface. It outlines the three-layer modular structure---Backend, Wrapper, and API---that bridges the gap between C-based RVV intrinsics and Python. This chapter details the available kernel wrappers, specialized configurations for convolutions, and provides guidelines for selecting optimal LMUL parameters to maximize hardware utilization.

\textbf{Chapter 6: Conclusion and Future Work} synthesizes the key findings of this project, reflecting on the effectiveness of the RISC-V Vector Extension for accelerating machine learning workloads. It summarizes the contributions made through the systematic framework and the RaiVeX Library. Finally, it outlines promising directions for future research, including expansion into DSP domains, validation on physical hardware, and the integration of quantization support for edge deployment.

\textbf{Chapter 7: Code Listings} provides supporting materials, including sample code implementations and specialized kernel configurations used throughout the validation process.

Supporting materials are provided in the final sections, including a comprehensive reference list and a link to the official GitHub repository containing the full source code for the RaiVeX Library.