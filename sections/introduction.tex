% !TEX root = ../Thesis.tex
\section{Introduction}

\subsection{Motivation}

The rapid evolution of artificial intelligence (AI) and digital signal processing (DSP) applications has fundamentally transformed the computational requirements of modern computing systems. AI workloads, particularly deep learning models, demand massive parallel computation for operations such as matrix multiplication, convolution, and tensor operations. Similarly, DSP applications require intensive mathematical operations including filtering, Fourier transforms, and correlation analysis. These computational patterns share a common characteristic: they involve highly parallel, data-intensive operations that can benefit significantly from vectorized execution.

% FLOPs growth figure here
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/Flops.png}
	\caption[Growth in AI model computational requirements]{Exponential growth in AI model computational requirements over time, showing the dramatic increase in FLOPs required for training state-of-the-art models.\cite{ourworldindata}}
    \label{fig:ai-growth}
\end{figure}

Traditional scalar processors, designed primarily for sequential instruction execution, face significant challenges when processing these data-parallel workloads. The von Neumann architecture, with its single instruction stream operating on individual data elements, creates a fundamental bottleneck for AI and DSP applications. For example, a typical convolution operation in a convolutional neural network (CNN) involves millions of multiply-accumulate operations that could theoretically be executed in parallel, but scalar processors must execute them sequentially, leading to substantial performance degradation.

The performance gap becomes even more pronounced when considering the memory bandwidth requirements of AI and DSP applications. These workloads often involve large datasets that exceed the capacity of processor caches, leading to frequent memory accesses. The arithmetic intensity---the ratio of computation to memory access---of many AI and DSP kernels is relatively low, meaning that processors spend significant time waiting for data rather than performing useful computation. This phenomenon, commonly referred to as the ``memory wall,'' represents a fundamental challenge for data-intensive computing.

\subsection{Limitations of Current Solutions}

Current solutions to these challenges have primarily relied on specialized hardware architectures and proprietary vector processing extensions. Graphics Processing Units (GPUs) have become the de facto standard for AI acceleration due to their thousands of parallel cores designed for data-parallel computation. However, GPUs present several limitations for AI and DSP applications:

\begin{itemize}
    \item \textbf{Power consumption}: GPUs consume significant power, making them unsuitable for edge computing and mobile applications where energy efficiency is paramount.
    \item \textbf{Programming complexity}: The GPU programming model, while powerful, requires specialized knowledge (CUDA, OpenCL) and often results in complex code that is difficult to optimize and maintain.
    \item \textbf{Integration challenges}: GPUs are discrete components requiring separate memory spaces and PCIe communication, introducing latency and bandwidth constraints for certain workloads.
\end{itemize}

Proprietary vector processing solutions, such as Intel's Advanced Vector Extensions (AVX) and ARM's NEON, provide another approach to accelerating data-parallel workloads. These extensions add vector processing capabilities to traditional CPU architectures, allowing multiple data elements to be processed with a single instruction (SIMD---Single Instruction, Multiple Data). However, these solutions have significant drawbacks that limit their effectiveness and adoption:

\begin{enumerate}
    \item \textbf{Vendor lock-in}: Proprietary vector extensions create situations where software optimized for one vendor's vector instructions cannot efficiently run on competitors' hardware, fragmenting the software ecosystem.
    
    \item \textbf{Fixed vector widths}: These extensions typically use fixed vector widths (e.g., 128-bit for NEON, 256-bit or 512-bit for AVX), meaning that software must be written for specific vector lengths and may not efficiently utilize processors with different vector capabilities.
    
    \item \textbf{Licensing costs}: Licensing costs and restrictions associated with proprietary architectures can be prohibitive, particularly for smaller companies and research institutions developing specialized AI and DSP applications.
    
    \item \textbf{Limited extensibility}: The closed nature of proprietary ISAs makes it difficult for researchers and developers to experiment with custom instructions or architectural modifications.
\end{enumerate}

\subsection{The RISC-V Vector Extension as a Solution}

RISC-V Vector Extensions (RVV) emerged as a promising solution to address these challenges by providing an open-source, royalty-free vector processing architecture specifically designed for data-parallel computation \cite{riscv-v-spec}. Unlike proprietary alternatives, RVV is developed through an open, collaborative process that ensures the architecture meets the diverse needs of the computing community.

The most distinctive feature of RVV is its \textbf{vector-length agnostic (VLA)} programming model, which represents a fundamental departure from traditional fixed-width SIMD approaches. In conventional vector processing, software must be written for specific vector widths, and different code paths are often required to support processors with different vector capabilities. RVV's vector-length agnostic model allows the same code to run efficiently across processors with different vector lengths, from embedded systems with short vectors to high-performance computing systems with very long vectors.

This flexibility is particularly valuable for AI and DSP applications, which span a wide range of computing environments with different performance and power requirements. An AI inference algorithm written using RVV can run efficiently on both a power-constrained edge device with 128-bit vectors and a high-performance server processor with 2048-bit vectors, without requiring code modifications or recompilation. The ratification of RVV Version 1.0 in late 2021 provided a crucial guarantee of stability, signaling to the industry that the architecture was mature and ready for widespread adoption.

\subsection{Problem Statement}

Despite the architectural advantages of the RISC-V Vector Extension, developing optimized kernels for RVV remains a challenging task. New developers often face two major obstacles:

\begin{enumerate}
    \item \textbf{Lack of standardized workflows}: There is currently no unified methodology for vector kernel development that encompasses design, verification, and performance evaluation in a cohesive framework.
    
    \item \textbf{Limited guidance on verification and evaluation}: While performance measurement is essential to demonstrate the benefits of vectorization, ensuring functional correctness is equally critical, especially when kernels are applied in sensitive domains such as artificial intelligence or embedded systems.
\end{enumerate}

Furthermore, the absence of widely available RVV-capable silicon necessitates reliance on emulation and RTL simulation environments for development and validation. This creates additional complexity in establishing reproducible benchmarking methodologies that can provide meaningful performance insights.

\subsection{Project Objectives}

This thesis investigates the role and potential impact of RISC-V Vector Extensions in accelerating AI and DSP applications. The primary objectives of this research are:

\begin{enumerate}
    \item \textbf{Develop a systematic framework} for the design, verification, and performance evaluation of RISC-V vector kernels that integrates open-source tools into a reproducible workflow.
    
    \item \textbf{Implement a comprehensive library of optimized RVV kernels} categorized by their computational patterns:
    \begin{itemize}
        \item \textbf{Compute-Intensive Linear Operators:} Matrix multiplication (\textit{matmul}), fully connected layers (\textit{dense}), and both standard and transposed convolutions.
        \item \textbf{Pointwise Activation and Arithmetic:} Rectified Linear Unit (\textit{ReLU}), Leaky ReLU, bias addition, and element-wise tensor addition.
        \item \textbf{Statistical and Normalization Kernels:} Batch normalization and the Softmax probability function.
        \item \textbf{Spatial Reduction and Indexing:} Max pooling, as well as ONNX-style data movement operations including Gather, GatherElements, and ScatterElements.
        \item \textbf{Decision Kernels:} Post-processing operations such as Non-Maximum Suppression (NMS).
    \end{itemize}
    
    \item \textbf{Establish functional verification methodologies} using ONNX (Open Neural Network Exchange) \cite{onnx} as a golden reference framework, ensuring correctness through quantitative metrics such as Signal-to-Noise Ratio (SNR) and Maximum Absolute Error (MaxAbs).
    
    \item \textbf{Conduct cycle-accurate performance evaluation} using RTL simulation with the Vicuna \cite{vicuna} and Ara \cite{ara} vector coprocessors, quantifying the speedup achieved through vectorization over scalar implementations.
    
    \item \textbf{Analyze the trade-offs} between different architectural approaches (high-performance vs. embedded) and provide insights into optimal kernel design strategies for various deployment scenarios.
\end{enumerate}

\subsection{Project Contributions}

This thesis makes the following contributions to the field of RISC-V vector processing for machine learning and DSP applications:

\begin{enumerate}
    \item \textbf{A reproducible three-step framework} integrating kernel design using RVV C-intrinsics, functional verification against ONNX golden references, and cycle-accurate performance analysis using RTL simulation with Verilator. This framework provides a systematic methodology that can be adopted by other researchers and developers.
    
    \item \textbf{A library of optimized RVV kernels (RVV64\_Library)} implementing a wide array of operations for neural network inference and signal processing. The library demonstrates efficient use of advanced RVV features, including strip-mining loops, vector-length agnostic programming, LMUL configuration, and masked operations.
    
    \item \textbf{Performance characterization on a high-performance RTL platform:} Implementation and evaluation were conducted using \textbf{Ara}, a 64-bit vector coprocessor targeting application-class workloads. The study explores the impact of Ara's microarchitecture, including its configurable lane counts and full floating-point support, on kernel efficiency.
    
    \item \textbf{Quantitative analysis} demonstrating significant performance gains through vectorization. Experimental results on the Ara co-processor show substantial speedups ranging from $4\times$ to over $70\times$ compared to scalar baseline implementations, highlighting the efficiency of the RVV ISA for data-parallel workloads.
    
    \item \textbf{High-level language integration through Python wrappers:} The development of a bridging layer using the \texttt{ctypes} foreign function interface, allowing the execution of optimized C++ vector kernels directly from a Python environment. This enables high-level algorithmic development and testing while maintaining the raw performance and hardware-level control of C++ implementations.
\end{enumerate}

\subsection{Thesis Organization}

The remainder of this thesis is organized as follows to guide the reader from foundational concepts to our specific contributions:

\textbf{Chapter 2: Background and Related Work} establishes the necessary theoretical foundation, providing a detailed overview of the RISC-V ISA, its relevant extensions, and the architectural principles of the Vector (RVV) extension. This chapter reviews the vector-length agnostic programming model, control and status registers, and the rich instruction set that enables efficient data-parallel processing. Additionally, it examines foundational academic research on RVV for machine learning acceleration, including work on specialized processors and prior vectorization efforts.

\textbf{Chapter 3: Development Tools} outlines the practical tools and workflows established for implementation and validation. This includes the RISC-V GNU toolchain configuration \cite{riscv-gnu-toolchain}, QEMU emulator setup for functional testing \cite{qemu-riscv}, Ara RTL simulation environment, Docker containerization for reproducible development, and ONNX framework integration for functional verification.

\textbf{Chapter 4: Methodology} presents the systematic three-step framework that forms the core of this research: (1) kernel design using RVV C-intrinsics with vector-length agnostic programming, (2) functional verification against ONNX golden references using quantitative metrics (SNR and MaxAbs), and (3) cycle-accurate performance evaluation using RTL simulation with Verilator \cite{verilator}. This chapter details the kernel selection criteria, vectorization design approach, verification workflow, and performance measurement techniques.

\textbf{Chapter 5: Library} introduces the RVV64\_Library, our collection of optimized RISC-V vector kernels for machine learning and DSP applications. This chapter systematically presents each implemented kernel---including matrix operations, activation functions, convolutional layers, and data movement operations---with detailed explanations of the naive sequential approach, proposed RVV-based solution, and performance optimization strategies.

\textbf{Chapter 6: Hardware (RTL Cores)} details the RTL platform used for cycle-accurate performance evaluation: Ara. This chapter explains the role of RTL simulation in architectural research, describes the microarchitectural details of the Ara coprocessor, and discusses the hardware-software interface for vector offloading.

\textbf{Chapter 7: Results and Discussion} presents both theoretical analysis and empirical evaluation of the implemented kernels. This chapter provides complexity analysis, theoretical speedup calculations, and experimental results from RTL simulation. Quantitative performance improvements are analyzed, including achieved speedups, functional unit utilization, and the impact of different LMUL configurations. The discussion interprets these results in the context of ML/DSP application deployment scenarios.

\textbf{Chapter 8: Conclusion} summarizes the key findings of this research, reflecting on the demonstrated effectiveness of the RISC-V Vector Extension for accelerating data-parallel workloads. This chapter synthesizes the contributions made through our systematic framework, optimized kernel library, and comprehensive performance characterization.

\textbf{Chapter 9: Future Work} outlines promising directions for extending this research, including algorithmic expansion to additional ML operators (pooling, normalization, attention mechanisms), hardware-level validation on FPGA platforms, and integration with established ML frameworks such as TensorFlow Lite or ONNX Runtime.

Supporting materials are provided in the final sections, including a comprehensive reference list, sample code implementations in the appendix, and a link to the official GitHub repository containing the full source code for the RVV64\_Library.