% !TEX root = ../Thesis.tex
\section{Introduction}

\subsection{Motivation}

The rapid evolution of artificial intelligence (AI) and digital signal processing (DSP) applications has fundamentally transformed the computational requirements of modern computing systems. AI workloads, particularly deep learning models, demand massive parallel computation for operations such as matrix multiplication, convolution, and tensor operations. Similarly, DSP applications require intensive mathematical operations including filtering, Fourier transforms, and correlation analysis. These computational patterns share a common characteristic: they involve highly parallel, data-intensive operations that can benefit significantly from vectorized execution.

% FLOPs growth figure here
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/Flops.png}
    \caption{Exponential growth in AI model computational requirements over time, showing the dramatic increase in FLOPs required for training state-of-the-art models.}
    \label{fig:ai-growth}
\end{figure}

Traditional scalar processors, designed primarily for sequential instruction execution, face significant challenges when processing these data-parallel workloads. The von Neumann architecture, with its single instruction stream operating on individual data elements, creates a fundamental bottleneck for AI and DSP applications. For example, a typical convolution operation in a convolutional neural network (CNN) involves millions of multiply-accumulate operations that could theoretically be executed in parallel, but scalar processors must execute them sequentially, leading to substantial performance degradation.

The performance gap becomes even more pronounced when considering the memory bandwidth requirements of AI and DSP applications. These workloads often involve large datasets that exceed the capacity of processor caches, leading to frequent memory accesses. The arithmetic intensity---the ratio of computation to memory access---of many AI and DSP kernels is relatively low, meaning that processors spend significant time waiting for data rather than performing useful computation. This phenomenon, commonly referred to as the ``memory wall,'' represents a fundamental challenge for data-intensive computing.

\subsection{Limitations of Current Solutions}

Current solutions to these challenges have primarily relied on specialized hardware architectures and proprietary vector processing extensions. Graphics Processing Units (GPUs) have become the de facto standard for AI acceleration due to their thousands of parallel cores designed for data-parallel computation. However, GPUs present several limitations for AI and DSP applications:

\begin{itemize}
    \item \textbf{Power consumption}: GPUs consume significant power, making them unsuitable for edge computing and mobile applications where energy efficiency is paramount.
    \item \textbf{Programming complexity}: The GPU programming model, while powerful, requires specialized knowledge (CUDA, OpenCL) and often results in complex code that is difficult to optimize and maintain.
    \item \textbf{Integration challenges}: GPUs are discrete components requiring separate memory spaces and PCIe communication, introducing latency and bandwidth constraints for certain workloads.
\end{itemize}

Proprietary vector processing solutions, such as Intel's Advanced Vector Extensions (AVX) and ARM's NEON, provide another approach to accelerating data-parallel workloads. These extensions add vector processing capabilities to traditional CPU architectures, allowing multiple data elements to be processed with a single instruction (SIMD---Single Instruction, Multiple Data). However, these solutions have significant drawbacks that limit their effectiveness and adoption:

\begin{enumerate}
    \item \textbf{Vendor lock-in}: Proprietary vector extensions create situations where software optimized for one vendor's vector instructions cannot efficiently run on competitors' hardware, fragmenting the software ecosystem.
    
    \item \textbf{Fixed vector widths}: These extensions typically use fixed vector widths (e.g., 128-bit for NEON, 256-bit or 512-bit for AVX), meaning that software must be written for specific vector lengths and may not efficiently utilize processors with different vector capabilities.
    
    \item \textbf{Licensing costs}: Licensing costs and restrictions associated with proprietary architectures can be prohibitive, particularly for smaller companies and research institutions developing specialized AI and DSP applications.
    
    \item \textbf{Limited extensibility}: The closed nature of proprietary ISAs makes it difficult for researchers and developers to experiment with custom instructions or architectural modifications.
\end{enumerate}

\subsection{The RISC-V Vector Extension as a Solution}

RISC-V Vector Extensions (RVV) emerged as a promising solution to address these challenges by providing an open-source, royalty-free vector processing architecture specifically designed for data-parallel computation. Unlike proprietary alternatives, RVV is developed through an open, collaborative process that ensures the architecture meets the diverse needs of the computing community.

The most distinctive feature of RVV is its \textbf{vector-length agnostic (VLA)} programming model, which represents a fundamental departure from traditional fixed-width SIMD approaches. In conventional vector processing, software must be written for specific vector widths, and different code paths are often required to support processors with different vector capabilities. RVV's vector-length agnostic model allows the same code to run efficiently across processors with different vector lengths, from embedded systems with short vectors to high-performance computing systems with very long vectors.

This flexibility is particularly valuable for AI and DSP applications, which span a wide range of computing environments with different performance and power requirements. An AI inference algorithm written using RVV can run efficiently on both a power-constrained edge device with 128-bit vectors and a high-performance server processor with 2048-bit vectors, without requiring code modifications or recompilation. The ratification of RVV Version 1.0 in late 2021 provided a crucial guarantee of stability, signaling to the industry that the architecture was mature and ready for widespread adoption.

\subsection{Problem Statement}

Despite the architectural advantages of the RISC-V Vector Extension, developing optimized kernels for RVV remains a challenging task. New developers often face two major obstacles:

\begin{enumerate}
    \item \textbf{Lack of standardized workflows}: There is currently no unified methodology for vector kernel development that encompasses design, verification, and performance evaluation in a cohesive framework.
    
    \item \textbf{Limited guidance on verification and evaluation}: While performance measurement is essential to demonstrate the benefits of vectorization, ensuring functional correctness is equally critical, especially when kernels are applied in sensitive domains such as artificial intelligence or embedded systems.
\end{enumerate}

Furthermore, the absence of widely available RVV-capable silicon necessitates reliance on emulation and RTL simulation environments for development and validation. This creates additional complexity in establishing reproducible benchmarking methodologies that can provide meaningful performance insights.

\subsection{Research Objectives}

This thesis investigates the role and potential impact of RISC-V Vector Extensions in accelerating AI and DSP applications. The primary objectives of this research are:

\begin{enumerate}
    \item \textbf{Develop a systematic framework} for the design, verification, and performance evaluation of RISC-V vector kernels that integrates open-source tools into a reproducible workflow.
    
    \item \textbf{Implement optimized RVV kernels} for fundamental machine learning and DSP operations, including:
    \begin{itemize}
        \item Matrix operations: multiplication, transposition, addition
        \item Activation functions: ReLU, Softmax
        \item Convolutional layers
        \item Training kernels: linear regression gradient descent
    \end{itemize}
    
    \item \textbf{Establish functional verification methodologies} using ONNX (Open Neural Network Exchange) as a golden reference framework, ensuring correctness through quantitative metrics such as Signal-to-Noise Ratio (SNR) and Maximum Absolute Error.
    
    \item \textbf{Conduct cycle-accurate performance evaluation} using RTL simulation with the Ara and Vicuna vector coprocessors, quantifying the speedup achieved through vectorization over scalar implementations.
    
    \item \textbf{Analyze the trade-offs} between different architectural approaches (high-performance vs. embedded) and provide insights into optimal kernel design strategies for various deployment scenarios.
\end{enumerate}

\subsection{Research Contributions}

This thesis makes the following contributions to the field of RISC-V vector processing for machine learning and DSP applications:

\begin{enumerate}
    \item \textbf{A reproducible three-step framework} integrating kernel design using RVV C-intrinsics, functional verification against ONNX golden references, and cycle-accurate performance analysis using RTL simulation with Verilator. This framework provides a systematic methodology that can be adopted by other researchers and developers.
    
    \item \textbf{A library of optimized RVV kernels} (RVV64\_Library) implementing fundamental operations for neural network inference and signal processing. The library demonstrates efficient use of RVV features including strip-mining loops, vector length agnostic programming, LMUL configuration, and masked operations.
    
    \item \textbf{Comprehensive performance characterization} of vectorized kernels on two distinct RTL platforms:
    \begin{itemize}
        \item \textbf{Ara}: A high-performance 64-bit vector coprocessor targeting application-class workloads with configurable lane counts (2--16 lanes) and full floating-point support.
        \item \textbf{Vicuna}: A timing-predictable 32-bit vector coprocessor targeting embedded real-time systems with deterministic execution guarantees.
    \end{itemize}
    
    \item \textbf{Quantitative analysis} demonstrating significant speedups achieved through vectorization, with experimental results showing up to 3.6$\\times$ improvement for matrix multiplication and 2.6$\\times$ for ReLU activation compared to scalar implementations.
    
    \item \textbf{A containerized development environment} (Docker-based) ensuring reproducibility across different development systems and simplifying the onboarding process for future researchers working with RISC-V vector extensions.
\end{enumerate}

\subsection{Thesis Organization}

The remainder of this thesis is organized as follows to guide the reader from foundational concepts to our specific contributions:

\textbf{Chapter 2: Background and Related Work} establishes the necessary theoretical foundation, providing a detailed overview of the RISC-V ISA, its relevant extensions, and the architectural principles of the Vector (RVV) extension. This chapter reviews the vector-length agnostic programming model, control and status registers, and the rich instruction set that enables efficient data-parallel processing. Additionally, it examines foundational academic research on RVV for machine learning acceleration, including work on specialized processors and prior vectorization efforts.

\textbf{Chapter 3: Development Tools} outlines the practical tools and workflows established for implementation and validation. This includes the RISC-V GNU toolchain configuration, QEMU emulator setup for functional testing, Ara RTL simulation environment, Docker containerization for reproducible development, and ONNX framework integration for functional verification.

\textbf{Chapter 4: Methodology} presents the systematic three-step framework that forms the core of this research: (1) kernel design using RVV C-intrinsics with vector-length agnostic programming, (2) functional verification against ONNX golden references using quantitative metrics (SNR and MaxAbs), and (3) cycle-accurate performance evaluation using RTL simulation with Verilator. This chapter details the kernel selection criteria, vectorization design approach, verification workflow, and performance measurement techniques.

\textbf{Chapter 5: Library} introduces the RVV64\_Library, our collection of optimized RISC-V vector kernels for machine learning and DSP applications. This chapter systematically presents each implemented kernel---including matrix operations (multiplication, transposition), activation functions (ReLU, Softmax), convolutional layers, and training kernels (linear regression)---with detailed explanations of the naive sequential approach, proposed RVV-based solution, and performance optimization strategies.

\textbf{Chapter 6: Hardware (RTL Cores)} details the two RTL platforms used for cycle-accurate performance evaluation: Ara and Vicuna. This chapter explains the role of RTL simulation in architectural research, describes the microarchitectural details of each coprocessor (Ara for high-performance throughput-oriented workloads and Vicuna for timing-predictable embedded systems), and justifies their selection as representative platforms spanning the full spectrum of RISC-V vector implementations.

\textbf{Chapter 7: Results and Discussion} presents both theoretical analysis and empirical evaluation of the implemented kernels. This chapter provides complexity analysis, theoretical speedup calculations, and experimental results from RTL simulation on both Ara and Vicuna platforms. Quantitative performance improvements are analyzed, including achieved speedups, functional unit utilization, and the impact of different LMUL configurations. The discussion interprets these results in the context of ML/DSP application deployment scenarios.

\textbf{Chapter 8: Conclusion} summarizes the key findings of this research, reflecting on the demonstrated effectiveness of the RISC-V Vector Extension for accelerating data-parallel workloads. This chapter synthesizes the contributions made through our systematic framework, optimized kernel library, and comprehensive performance characterization across diverse hardware platforms.

\textbf{Chapter 9: Future Work} outlines promising directions for extending this research, including algorithmic expansion to additional ML operators (pooling, normalization, attention mechanisms), hardware-level validation on FPGA and ASIC platforms, integration with established ML frameworks (TensorFlow Lite, ONNX Runtime), and exploration of multi-core vector configurations for improved scalability.

Supporting materials are provided in the final sections, including acknowledgments of collaborators and advisors, a comprehensive reference list, and complete source code listings for all implemented kernels in the appendix.
