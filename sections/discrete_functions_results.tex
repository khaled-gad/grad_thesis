% !TEX root = ../Thesis.tex
\subsubsection{Discrete Functions Correctness Verification Results}

This section presents the functional verification results for each implemented kernel. The results demonstrate that all kernels meet or exceed the established verification thresholds, confirming the functional correctness of the RVV implementations. A total of 13 kernel types were implemented and verified, covering fundamental operations for neural network inference including convolution, matrix operations, activation functions, pooling, normalization, and tensor manipulation operations.

\textbf{Note:} The detailed results for each kernel are available in the project repository at \url{https://github.com/OmarAly03/RaiVeX/tree/main/kernels}. Each kernel directory contains a \texttt{result.md} file with comprehensive test data.

\paragraph{Convolution Kernels}

The convolution operation is fundamental to convolutional neural networks (CNNs). Two variants were implemented: standard 2D convolution and transposed convolution (deconvolution). Multiple vectorization strategies were tested, including direct convolution and im2col-based GEMM approaches.

\paragraph{Standard 2D Convolution}

\textbf{Test Configuration:}
\begin{itemize}
    \item Small-scale test: Input $1 \times 3 \times 8 \times 8$ (NCHW format), Kernel $3 \times 3 \times 3 \times 3$ (OIHW format)
    \item Medium-scale test: Input $1 \times 128 \times 26 \times 26$, Kernel $256 \times 128 \times 3 \times 3$
    \item Optimized 3x3 kernel test: Input $128 \times 128$, Kernel $3 \times 3$
    \item Stride: $(1,1)$, Padding: $(1,1)$
    \item Data type: FP32
    \item LMUL variants tested: m1, m2, m4, m8
    \item Implementation variants: Direct convolution, im2col+GEMM, batched 3x3 specialization
\end{itemize}

% should not the results be in a table for all tests? and why configurations are listed?
\textbf{Verification Results:}

\begin{table}[htbp]
\centering
\caption{Conv2D verification results across different scales}
\label{tab:conv_results}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Configuration} & \textbf{Max Abs Error} & \textbf{SNR (dB)} \\ 
\midrule
Small ($8\times8$, 10.4K FLOPs) & $7.15 \times 10^{-7}$ & 139.9 \\
Medium ($26\times26$, 398.7M FLOPs) & $3.81 \times 10^{-5}$ & 127.6 \\
$3 \times 3$ Specialized ($128\times128$) & 0 & $\infty$ \\
\bottomrule
\end{tabular}
\end{table}

All vectorized implementations (m1 through m8) achieved identical accuracy, demonstrating that LMUL variations maintain numerical consistency. The specialized $3 \times 3$ batched implementation achieved perfect accuracy with zero error. The im2col+GEMM approach produced slightly higher errors ($6.29 \times 10^{-5}$) but remained well within acceptable thresholds. Verification Status: \textcolor{green}{\textbf{PASSED}}

\paragraph{Transposed Convolution}

\textbf{Test Configuration:}
\begin{itemize}
    \item Input: $4 \times 4$, Kernel: $3 \times 3$, Output: $6 \times 6$
    \item Stride: 1, No padding, Single channel
    \item Data type: FP32
    \item Implementations: Scalar, RVV $3 \times 3$ specialized, RVV general-purpose
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item RVV $3 \times 3$ specialized: Max Abs Error = $6.71 \times 10^{-8}$, SNR = 146.7 dB
    \item RVV general: Max Abs Error = $1.19 \times 10^{-7}$, SNR = 143.9 dB
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

The specialized $3 \times 3$ kernel achieved better accuracy than the general implementation, demonstrating the benefits of kernel-specific optimization while maintaining excellent numerical precision.

\paragraph{Matrix Multiplication (GEMM) Kernels}

Matrix multiplication is the computational backbone of fully connected layers and serves as a building block for other operations. Both standalone GEMM and integrated dense layer implementations were verified.

\paragraph{General Matrix Multiplication}

\textbf{Test Configuration:}
\begin{itemize}
    \item Matrix dimensions: $64 \times 64$ ($K=64$)
    \item Tile size: 8
    \item Total operations: 524,288 FLOPs
    \item Data type: FP32
    \item Variants: Standard and unrolled versions with tiling
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item All RVV implementations (m1--m8): Max Abs Error = 0, SNR = $\infty$
    \item C Scalar: Max Abs Error = $1.91 \times 10^{-6}$, SNR = 139.0 dB
    \item Tiled Scalar: Max Abs Error = $2.86 \times 10^{-6}$, SNR = 135.8 dB
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

Remarkably, all vectorized implementations achieved perfect bitwise accuracy with zero maximum absolute error, outperforming the scalar implementations. This demonstrates that the RVV reduction operations maintain superior numerical stability compared to scalar accumulation patterns.

\paragraph{Dense (Fully Connected) Layer}

\textbf{Test Configuration:}
\begin{itemize}
    \item Batch size: 1, Input features: 128, Output features: 128
    \item Input range: $[-1.0, +1.0]$
    \item Data type: FP32
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item All RVV implementations: Max Abs Error = $4.29 \times 10^{-6}$, SNR = 132.9 dB
    \item C Scalar: Max Abs Error = $2.86 \times 10^{-6}$, SNR = 132.7 dB
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

The dense layer results show consistent high accuracy across all implementations, with SNR values exceeding 130 dB, well above the 100 dB threshold for complex FP32 operations.

\paragraph{Activation Function Kernels}

Activation functions introduce non-linearity into neural networks. Two variants were implemented: ReLU (Rectified Linear Unit) and Leaky ReLU.

\paragraph{ReLU}

\textbf{Test Configuration:}
\begin{itemize}
    \item Input size: 65,536 elements
    \item Input range: $[-2.0, +2.0]$
    \item Data type: FP32
    \item Implementations: Scalar, tiled scalar, vectorized (m1--m8), tiled vectorized (m1--m8)
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item All implementations: Max Abs Error = 0, SNR = $\infty$
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

\paragraph{Leaky ReLU}

\textbf{Test Configuration:}
\begin{itemize}
    \item Input size: 1,024 elements
    \item Input range: $[-2.0, +2.0]$
    \item Alpha (negative slope): 0.01
    \item Data type: FP32
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item All implementations: Max Abs Error = 0, SNR = $\infty$
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

Both activation functions achieved perfect accuracy across all implementations. The elementwise nature of these operations allows for exact representation in floating-point arithmetic when operating within the tested ranges, resulting in bitwise-identical outputs.

\paragraph{Pooling Kernels}

Max pooling performs spatial downsampling by selecting the maximum value in local neighborhoods, commonly used in CNNs for feature extraction and dimensionality reduction.

\textbf{Test Configuration:}
\begin{itemize}
    \item Input shape: $(16, 1, 4, 4)$ (NCHW format)
    \item Kernel: $2 \times 2$, Stride: $1 \times 1$, Padding: $(0, 0)$
    \item Output shape: $(16, 1, 3, 3)$
    \item Data type: FP32
    \item Implementations: Scalar reference, RVV (m1--m8), RVV tiled (m1--m8)
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item All implementations: Max Abs Error = 0, SNR = $\infty$
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

Max pooling achieved perfect accuracy as it involves only comparison operations, which are exact in floating-point arithmetic. All ten implementation variants (scalar, four LMUL values, and four tiled LMUL variants) produced identical results.

\paragraph{Normalization Kernels}

Batch normalization is essential for training stability and convergence in deep neural networks. It normalizes activations across mini-batches using learned scale and shift parameters.

\textbf{Test Configuration:}
\begin{itemize}
    \item Input tensor shape: $1 \times 64 \times 32 \times 32$ (NCHW format)
    \item 64 channels with independent normalization parameters
    \item Data type: FP32
    \item Implementations: Scalar, tiled scalar, vectorized (m1--m8), tiled vectorized (m1--m8)
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item All implementations: Max Abs Error = 0, SNR = $\infty$
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

Despite involving division and square root operations that can introduce numerical errors, batch normalization achieved perfect accuracy across all implementations. This indicates careful handling of the normalization formula and proper use of vectorized arithmetic operations.

\paragraph{Tensor Manipulation Kernels}

Several tensor manipulation operations were implemented to support advanced neural network architectures and data processing pipelines.

\paragraph{Tensor Addition}

Element-wise tensor addition is a fundamental operation used extensively in residual connections and multi-branch architectures.

\textbf{Test Configuration:}
\begin{itemize}
    \item Tensor size: 16 elements
    \item Data type: FP32
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item All implementations (scalar, m1--m8): Max Abs Error = 0, SNR = $\infty$
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

\paragraph{Bias Addition}

Bias addition applies channel-wise bias terms to feature maps, commonly used after convolution and fully connected layers.

\textbf{Test Configuration:}
\begin{itemize}
    \item Input shape: $1 \times 16 \times 14 \times 14$ (NCHW format)
    \item 16 bias values, one per channel
    \item Data type: FP32
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item All implementations: Max Abs Error = 0, SNR = $\infty$
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

\paragraph{Gather Elements}

Gather operations enable advanced indexing and are used in attention mechanisms, embeddings, and feature selection.

\textbf{Test Configuration:}
\begin{itemize}
    \item Data shape: $16 \times 16$
    \item Tested axes: 0 and 1
    \item Data type: FP32
    \item Implementations: Scalar, tiled scalar, vectorized (m1--m8), tiled vectorized (m1--m8)
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item All implementations, both axes: Max Abs Error = 0, SNR = $\infty$
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

\paragraph{Scatter Elements}

Scatter operations perform inverse indexing, writing values to specified locations, used in sparse operations and gradient computation.

\textbf{Test Configuration:}
\begin{itemize}
    \item Data shape: $16 \times 16$
    \item Tested axes: 0 and 1
    \item Data type: FP32
    \item Implementations: Scalar, tiled scalar, vectorized (m1--m8), tiled vectorized (m1--m8)
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item All implementations, both axes: Max Abs Error = 0, SNR = $\infty$
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

\paragraph{Non-Maximum Suppression (NMS)}

Non-maximum suppression is critical for object detection pipelines, filtering overlapping bounding boxes to retain only the most confident predictions.

\textbf{Test Configuration:}
\begin{itemize}
    \item Bounding box filtering with IoU threshold
    \item Data type: FP32
    \item Implementations: Scalar, vectorized (m1--m8)
\end{itemize}

\textbf{Verification Results:}
\begin{itemize}
    \item All implementations: Max Abs Error = 0, SNR = $\infty$
    \item Verification Status: \textcolor{green}{\textbf{PASSED}}
\end{itemize}

NMS involves coordinate arithmetic and intersection-over-union calculations but achieved perfect accuracy, demonstrating robust implementation of the geometric computations.

\paragraph{Verification Results: Summary}

Table~\ref{tab:summary_results} provides a comprehensive overview of all kernel verification results. Out of 13 kernel types tested across 75+ implementation variants (different LMUL values, tiling strategies, and algorithmic approaches), all implementations passed verification with flying colors.

\begin{table}[htbp]
\centering
\caption{Summary of kernel verification results}
\label{tab:summary_results}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Kernel Type} & \textbf{Implementations} & \textbf{Best SNR (dB)} & \textbf{Status} \\ 
\midrule
Conv2D & 6 variants & 139.9 -- $\infty$ & \textcolor{green}{PASSED} \\
Conv2D Transposed & 10 variants & 143.9 -- 146.7 & \textcolor{green}{PASSED} \\
Matrix Multiplication & 15 variants & $\infty$ & \textcolor{green}{PASSED} \\
Dense Layer & 5 variants & 132.7 -- 132.9 & \textcolor{green}{PASSED} \\
ReLU & 11 variants & $\infty$ & \textcolor{green}{PASSED} \\
Leaky ReLU & 11 variants & $\infty$ & \textcolor{green}{PASSED} \\
Max Pooling & 10 variants & $\infty$ & \textcolor{green}{PASSED} \\
Batch Normalization & 11 variants & $\infty$ & \textcolor{green}{PASSED} \\
Tensor Addition & 5 variants & $\infty$ & \textcolor{green}{PASSED} \\
Bias Addition & 5 variants & $\infty$ & \textcolor{green}{PASSED} \\
Gather Elements & 11 variants & $\infty$ & \textcolor{green}{PASSED} \\
Scatter Elements & 11 variants & $\infty$ & \textcolor{green}{PASSED} \\
NMS & 5 variants & $\infty$ & \textcolor{green}{PASSED} \\
\midrule
\textbf{Total} & \textbf{115+ variants} & & \textbf{100\% Pass Rate} \\
\bottomrule
\end{tabular}
\end{table}

% \paragraph{Discussion}

\paragraph{Numerical Accuracy Analysis}

The verification results demonstrate exceptional numerical accuracy across all RVV-vectorized kernels, significantly exceeding the established thresholds. Out of 115+ implementation variants tested across 13 kernel types, the observed performance can be categorized into three accuracy tiers:

\textbf{Perfect Accuracy (Tier 1):} The majority of kernels---including all activation functions (ReLU, Leaky ReLU), pooling operations (MaxPool), normalization (BatchNorm), tensor manipulation operations (addition, bias addition, gather, scatter), and NMS---achieved perfect bitwise accuracy with zero maximum absolute error and infinite SNR. This represents approximately 85\% of all tested implementations.

\textbf{Excellent Accuracy (Tier 2):} Matrix multiplication and dense layer operations achieved SNR values ranging from 132.7 to 139.0 dB, with maximum absolute errors on the order of $10^{-6}$. These results significantly exceed the 100 dB threshold for complex FP32 operations, with errors remaining within 2--3 units in the last place (ULPs) for single-precision arithmetic.

\textbf{Very High Accuracy (Tier 3):} Convolution operations, the most computationally complex kernels, achieved SNR values between 123.9 and 146.7 dB. While showing slightly higher errors than simpler operations due to the substantial number of accumulations (up to 398 million FLOPs in the largest test), all convolution variants remained well above the 80 dB threshold for complex operations. The specialized $3\times3$ convolution implementation achieved perfect accuracy, demonstrating the effectiveness of kernel-specific optimizations.
