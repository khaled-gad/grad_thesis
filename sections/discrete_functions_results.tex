% !TEX root = ../Thesis.tex
\subsubsection{Discrete Functions Correctness Verification Results}

This section presents the functional verification results for each implemented kernel. The results demonstrate that all kernels meet or exceed the established verification thresholds, confirming the functional correctness of the RVV implementations. A total of 13 kernel types were verified across 115+ implementation variants, covering fundamental operations for neural network inference including convolution, matrix operations, activation functions, pooling, normalization, and tensor manipulation operations.

All implementations were validated against ONNX Runtime golden references using the dual-path verification methodology described in Section 3.4.2. Table~\ref{tab:verification_summary} provides an overview of all verified kernels, showing universal success with a 100\% pass rate. \\

\textbf{Note:} The detailed results for each kernel are available in the project repository at \url{https://github.com/OmarAly03/RaiVeX/tree/main/kernels}. Each kernel directory contains a \texttt{result.md} file with comprehensive test data.


\begin{table}[H]
\centering
\caption{Summary of kernel verification results across all implementations}
\label{tab:verification_summary}
\vspace{1em}
\begin{tabular}{l c c c }
\toprule
\textbf{Kernel Type}  & \textbf{Max Abs Error} & \textbf{SNR (dB)} & \textbf{Status} \\ 
\midrule
Conv2D                   & $6.48 \times 10^{-5}$ & 123.9 -- 139.9 & \textcolor{green}{ PASSED} \\
Conv2D Transposed       & $1.19 \times 10^{-7}$ & 143.2 -- 146.7 & \textcolor{green}{ PASSED} \\
Conv2D $3\times3$ Specialized  & 0                     & $\infty$        & \textcolor{green}{ PASSED} \\
Matrix Multiplication   & $2.86 \times 10^{-6}$ & 135.8 -- $\infty$ & \textcolor{green}{ PASSED} \\
Dense Layer              & $4.29 \times 10^{-6}$ & 132.7 -- 132.9 & \textcolor{green}{ PASSED} \\
ReLU                    & 0                     & $\infty$        & \textcolor{green}{ PASSED} \\
Leaky ReLU              & 0                     & $\infty$        & \textcolor{green}{ PASSED} \\
Max Pooling             & 0                     & $\infty$        & \textcolor{green}{ PASSED} \\
Batch Normalization     & 0                     & $\infty$        & \textcolor{green}{ PASSED} \\
Tensor Addition          & 0                     & $\infty$        & \textcolor{green}{ PASSED} \\
Bias Addition            & 0                     & $\infty$        & \textcolor{green}{ PASSED} \\
Gather Elements         & 0                     & $\infty$        & \textcolor{green}{ PASSED} \\
Scatter Elements        & 0                     & $\infty$        & \textcolor{green}{ PASSED} \\
NMS                      & 0                     & $\infty$        & \textcolor{green}{ PASSED} \\
\midrule
\textbf{}          & & & \textbf{100\% Pass Rate} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Perfect Accuracy Kernels} \\

Nine kernel types achieved perfect bitwise accuracy with zero maximum absolute error and infinite SNR across all implementation variants: ReLU, Leaky ReLU, Max Pooling, Batch Normalization, Tensor Addition, Bias Addition, Gather Elements, Scatter Elements, NMS, and the specialized Conv2D $3\times3$ kernel. This represents approximately 85\% of all tested implementations. 

These kernels achieved perfect accuracy due to their computational characteristics—either involving only elementwise operations with minimal accumulation (activation functions, additive operations), comparison-based selection (pooling), or carefully controlled normalization and indexing operations. All LMUL configurations (m1, m2, m4, m8) and optimization strategies (tiled, non-tiled) produced identical results for these kernels.

\textbf{Convolution Kernels}

Convolution operations showed varying accuracy depending on scale and implementation approach. Table~\ref{tab:conv_detailed} summarizes the results across different configurations.

\begin{table}[H]
\centering
\caption{Conv2D verification results across scales and implementations}
\label{tab:conv_detailed}
\vspace{1em}
\begin{tabular}{l l c c}
\toprule
\textbf{Configuration} & \textbf{Implementation} & \textbf{Max Abs Error} & \textbf{SNR (dB)} \\ 
\midrule
\multirow{2}{*}{Small ($8\times8$)}     
                & All RVV variants        & $7.15 \times 10^{-7}$ & 139.9 \\
                & Scalar baseline         & $9.54 \times 10^{-7}$ & 139.0 \\
\midrule
\multirow{3}{*}{Medium ($26\times26$)}
                & All RVV variants        & $3.81 \times 10^{-5}$ & 127.6 \\
                & IM2COL+GEMM            & $6.29 \times 10^{-5}$ & 124.0 \\
                & Scalar baseline         & $6.48 \times 10^{-5}$ & 123.9 \\
\midrule
\multirow{1}{*}{$3\times3$ Specialized ($128\times128$)}
                & All variants  & 0                     & $\infty$ \\
\bottomrule
\end{tabular}
\end{table}

All RVV vectorized implementations (m1 through m8) achieved identical accuracy within each configuration, demonstrating LMUL-independent numerical consistency. Notably, RVV implementations outperformed scalar baselines for both small and medium scales—at medium scale, RVV achieved errors 41\% lower than scalar ($3.81 \times 10^{-5}$ vs $6.48 \times 10^{-5}$) with 3.7 dB better SNR. The im2col+GEMM approach produced slightly higher errors than direct RVV convolution but remained well within acceptable thresholds.

The specialized $3\times3$ kernel achieved perfect accuracy across all eight variants (m1/m2/m4/m8 in both batched and non-batched configurations), demonstrating the benefits of kernel-specific optimization.


\textbf{Transposed Convolution} \\

Transposed convolution showed excellent accuracy across three different input scales, with implementation performance varying by problem size. Table~\ref{tab:conv_transposed} summarizes the results.

\begin{table}[H]
\centering
\caption{Transposed convolution verification across scales ($3\times3$ kernel, stride=1)}
\label{tab:conv_transposed}
\vspace{1em}
\begin{tabular}{l l c c}
\toprule
\textbf{Scale} & \textbf{Implementation Type} & \textbf{Max Abs Error} & \textbf{SNR (dB)} \\ 
\midrule
\multirow{3}{*}{Small ($4\times4$)} 
    & RVV $3\times3$ Specialized (m1--m8)  & $6.71 \times 10^{-8}$ & 146.7 \\
    & RVV General Purpose (m1--m8)  & $1.19 \times 10^{-7}$ & 143.9 \\
    & Scalar Baseline               & $1.19 \times 10^{-7}$ & 143.2 \\
\midrule
\multirow{3}{*}{Medium ($26\times26$)} 
    & RVV General Purpose (m1--m8)  & $4.77 \times 10^{-7}$ & 141.6 \\
    & RVV $3\times3$ Specialized (m1--m8)  & $4.77 \times 10^{-7}$ & 140.9--141.2 \\
    & Scalar Baseline               & $4.77 \times 10^{-7}$ & 141.3 \\
\midrule
\multirow{3}{*}{Large ($128\times128$)} 
    & RVV General Purpose (m1--m8)  & $7.15 \times 10^{-7}$ & 141.4 \\
    & RVV $3\times3$ Specialized (m1--m8)  & $4.77 \times 10^{-7}$ & 141.2--141.3 \\
    & Scalar Baseline               & $4.77 \times 10^{-7}$ & 141.6 \\
\bottomrule
\end{tabular}
\end{table}

At small scale ($4\times4$ input), the specialized $3\times3$ kernel achieved 43\% lower error than both general RVV and scalar implementations ($6.71 \times 10^{-8}$ vs $1.19 \times 10^{-7}$) with 2.8--3.5 dB better SNR. However, this advantage diminishes at larger scales.

For medium and large scales ($26\times26$ and $128\times128$), all implementations converged to similar accuracy levels, with maximum absolute errors ranging from $4.77 \times 10^{-7}$ to $7.15 \times 10^{-7}$ and SNR values between 140.9--141.6 dB. At these scales, scalar baseline performed comparably to or slightly better than RVV implementations, though all variants remained well above the 80 dB threshold for complex FP32 operations.

All four LMUL variants (m1--m8) within each implementation type produced nearly identical results, with SNR variations under 0.5 dB at medium/large scales—indicating LMUL choice affects performance but not accuracy.

\textbf{Matrix Multiplication and Dense Layer} \\

Matrix multiplication demonstrated remarkable numerical properties, with RVV implementations achieving superior accuracy to scalar baselines.

\begin{table}[H]
\centering
\caption{Matrix multiplication verification ($64\times64$)}
\label{tab:matmul_detailed}
\vspace{1em}
\begin{tabular}{l c c}
\toprule
\textbf{Implementation Type} & \textbf{Max Abs Error} & \textbf{SNR (dB)} \\ 
\midrule
All RVV Variants & 0 & $\infty$ \\
Scalar Baseline   & $1.91 \times 10^{-6}$ & 139.0 \\
Tiled Scalar  & $2.86 \times 10^{-6}$ & 135.8 \\
\bottomrule
\end{tabular}
\end{table}

All twelve RVV implementations achieved perfect bitwise accuracy with zero error, significantly outperforming scalar implementations. This superior performance suggests that RVV reduction instructions employ hardware-optimized compensated summation at the microarchitectural level. All LMUL values (m1--m8) and optimization strategies (standard, unrolled, tiled) produced identical perfect results.

Dense layer (fully connected) operations showed consistent high accuracy across implementations.

\begin{table}[H]
\centering
\caption{Dense layer verification (batch=1, $128\times128$)}
\label{tab:dense_detailed}
\vspace{1em}
\begin{tabular}{l c c}
\toprule
\textbf{Implementation Type} & \textbf{Max Abs Error} & \textbf{SNR (dB)} \\ 
\midrule
All RVV Variants (m1--m8, 4 total) & $4.29 \times 10^{-6}$ & 132.9 \\
Scalar Baseline                     & $2.86 \times 10^{-6}$ & 132.7 \\
\bottomrule
\end{tabular}
\end{table}

Both scalar and RVV implementations achieved SNR $>$ 132 dB, well above the 100 dB threshold. All LMUL variants produced identical results, with RVV showing slightly higher error than scalar but maintaining excellent accuracy for this matrix-vector multiplication pattern.


\textbf{Numerical Accuracy Analysis} \\

The verification results demonstrate exceptional numerical accuracy across all RVV-vectorized kernels, significantly exceeding the established thresholds. Out of 115+ implementation variants tested across 13 kernel types, the observed performance can be categorized into three accuracy tiers:

\paragraph{Perfect Accuracy (Tier 1):} The majority of kernels---including all activation functions (ReLU, Leaky ReLU), pooling operations (MaxPool), normalization (BatchNorm), tensor manipulation operations (addition, bias addition, gather, scatter), and NMS---achieved perfect bitwise accuracy with zero maximum absolute error and infinite SNR. This represents approximately 85\% of all tested implementations. The perfect accuracy results from the elementwise or comparison-based nature of these operations, where each output depends on a small, fixed number of inputs, avoiding error accumulation.

\paragraph{Excellent Accuracy (Tier 2):} Matrix multiplication achieved zero error for all RVV implementations while scalar implementations showed errors up to $2.86 \times 10^{-6}$ (SNR = 135.8 dB). This superior RVV performance suggests that hardware-optimized reduction instructions employ compensated summation techniques at the microarchitectural level. Dense layers achieved SNR values of 132.7--132.9 dB with errors on the order of $10^{-6}$. These results significantly exceed the 100 dB threshold for complex FP32 operations, with errors remaining within 2--3 units in the last place (ULPs) for single-precision arithmetic.

\paragraph{Very High Accuracy (Tier 3):} Convolution operations, the most computationally complex kernels, achieved SNR values between 123.9 and 146.7 dB. While showing slightly higher errors than simpler operations due to the substantial number of accumulations (up to 398 million FLOPs in the largest test), all convolution variants remained well above the 80 dB threshold for complex operations. The im2col+GEMM approach produced slightly higher errors than direct convolution but remained well within acceptable bounds; whereas the specialized $3\times3$ convolution implementation achieved perfect accuracy, demonstrating the effectiveness of kernel-specific optimizations.

A critical finding is the \textbf{LMUL-independent accuracy}: all LMUL configurations (m1, m2, m4, m8) produced numerically identical results within each kernel. This demonstrates that vector register grouping affects only performance, not correctness, allowing developers to optimize for throughput without accuracy concerns.

% what is this textbf trying to add? is this placement appropriate?
% \textbf{Comparison with ONNX Runtime}

% The selection of ONNX Runtime as the golden reference was based on several considerations. ONNX Runtime is widely recognized as a high-performance, production-grade inference engine that implements the ONNX specification accurately and is extensively tested across various platforms, hardware accelerators (CPU, GPU, NPU), and deployment scenarios. It serves as a de facto standard for cross-platform ML inference. By comparing against ONNX Runtime, we ensure that our implementations conform to industry-standard ML operator semantics and produce results compatible with models trained using popular frameworks (PyTorch, TensorFlow, etc.).
