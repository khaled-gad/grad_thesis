\section{Conclusion and Future Work}

\subsection{Conclusion}

This thesis presented the design, implementation, and evaluation of RaiVeX Library, a comprehensive collection of RISC-V Vector Extension (RVV 1.0) accelerated kernels targeting machine learning and high-performance computing workloads. The research addressed a significant gap in the existing literature concerning the availability of production-ready, thoroughly benchmarked vectorized implementations of fundamental ML primitives on the RISC-V architecture. The library encompasses a diverse range of computational kernels organized into six categories: compute-intensive linear operators (matrix multiplication, convolution, transposed convolution, dense layers), pointwise activation and arithmetic kernels (ReLU, Leaky ReLU, bias addition, tensor addition), statistical and normalization kernels (batch normalization, softmax), spatial reduction kernels (max pooling), tensor indexing and data movement operations (gather, gather elements, scatter elements), and post-processing kernels (non-maximum suppression). Each kernel was implemented in C++ utilizing RVV intrinsics with systematic exploration of different LMUL configurations (M1, M2, M4, M8), enabling fine-grained control over vector register utilization and performance optimization. The architectural design emphasizes modularity and reusability through a low-level vector API abstraction layer that encapsulates common RVV operations including vector loads, stores, reductions, multiply-accumulate, and mask operations. Functional correctness was rigorously validated against ONNX golden references using QEMU emulation, while performance benchmarking on the Ara vector co-processor demonstrated substantial speedups ranging from 4$\times$ to over 70$\times$ compared to scalar baseline implementations. The practical applicability of the library was validated through end-to-end inference implementations of LeNet-5 and Tiny-YOLOv2 neural networks, demonstrating that the vectorized kernels integrate seamlessly into real deep learning pipelines. These results collectively establish that the RISC-V Vector Extension, when properly optimized, constitutes a viable and high-performance alternative for accelerating machine learning inference on resource-constrained embedded platforms, contributing both a practical software artifact and empirical evidence to the growing body of research on open-source hardware architectures for AI acceleration.

\subsection{Future Work}

While RaiVeX Library demonstrates significant performance improvements and practical applicability, several avenues for future research and development remain open. The following subsections outline potential extensions, improvements, and deployment considerations that could further enhance the library's capabilities and broaden its impact.

\subsubsection{Extension to Additional Workload Domains}

The current implementation focuses primarily on machine learning kernels; however, the underlying vector primitives and optimization strategies developed in this work are directly applicable to other computationally intensive domains. Future efforts could extend the library to encompass DSP workloads, including Fast Fourier Transform (FFT), Finite Impulse Response (FIR) and Infinite Impulse Response (IIR) filters, and correlation operations that are fundamental to audio processing, telecommunications, and radar applications. Similarly, scientific computing kernels such as sparse matrix operations, linear algebra decompositions (LU, QR, Cholesky), and numerical integration routines would benefit from RVV acceleration. Image and video processing primitives---including color space conversion, histogram computation, morphological operations, and compression algorithms---represent another natural extension. Such diversification would position RaiVeX Library as a general-purpose high-performance computing library for the RISC-V ecosystem rather than being limited to ML workloads alone.

\subsubsection{Deployment on Physical RISC-V Hardware}

A critical next step involves validating and benchmarking the library on physical RISC-V hardware that supports the RVV 1.0 specification. While the Ara vector co-processor provides valuable performance insights through cycle-accurate simulation, deployment on actual silicon would yield more realistic performance metrics accounting for real-world factors such as memory bandwidth limitations, cache behavior, thermal constraints, and power consumption. Emerging RISC-V processors with vector support, including commercial offerings and development boards, present opportunities for such validation. Hardware deployment would also enable energy efficiency measurements, which are particularly relevant for the embedded and edge computing use cases that RISC-V targets. Furthermore, testing on diverse hardware implementations with varying vector lengths (VLEN) would verify the library's scalability and portability across different RISC-V platforms.

\subsubsection{Enhancement of the Python Interface and Abstraction Layer}

The current Python bindings provide functional access to the vectorized kernels through ctypes wrappers around shared libraries; however, significant improvements could enhance usability and developer experience. Future work should focus on developing a higher-level Pythonic API that abstracts memory management, pointer handling, and data type conversions, enabling users to interact with the library using native NumPy arrays without explicit pointer manipulation. Integration with popular deep learning frameworks such as PyTorch or TensorFlow through custom operator registration would allow seamless incorporation of RVV-accelerated kernels into existing ML workflows. Additionally, implementing automatic kernel selection based on input dimensions and available hardware capabilities, along with comprehensive documentation, type hints, and error handling, would significantly lower the barrier to adoption for researchers and practitioners unfamiliar with low-level systems programming.

\subsubsection{Kernel Optimization and Algorithmic Improvements}

While the implemented kernels demonstrate substantial speedups, further optimization opportunities exist. Advanced techniques such as cache-aware tiling strategies, software pipelining, and loop unrolling specifically tuned for different VLEN configurations could yield additional performance gains. For convolution operations, exploring alternative algorithms such as Winograd-based approaches or more sophisticated im2col-GEMM implementations optimized for specific kernel sizes may prove beneficial. Quantization support for INT8 and INT4 data types, which are increasingly prevalent in edge ML deployment, would enable even greater throughput by leveraging the vector unit's ability to process more elements per instruction. Auto-tuning frameworks that systematically explore the optimization space for given input dimensions and hardware configurations represent another promising direction.

\subsubsection{Expanded Neural Network Model Support}

The successful implementation of LeNet-5 and Tiny-YOLOv2 demonstrates the library's capability to support complete inference pipelines. Future work could expand this to encompass a broader range of neural network architectures, including transformer-based models that have become prevalent in natural language processing and computer vision. Implementing attention mechanisms, layer normalization, and Gaussian Error Linear Unit (GELU) activations would enable support for models such as Bidirectional Encoder Representations from Transformers (BERT), Generative Pre-trained Transformer (GPT) variants, and Vision Transformers. Additionally, developing an ONNX runtime backend that automatically maps supported operators to RVV-accelerated kernels would provide a standardized interface for deploying arbitrary trained models, significantly expanding the library's practical utility in production environments.
