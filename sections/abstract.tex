
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

The proliferation of machine learning (ML) workloads across edge devices and embedded systems has intensified the demand for energy-efficient, high-performance computing architectures beyond conventional GPU-dominated paradigms. RISC-V, an open-source instruction set architecture, has emerged as a promising alternative, with its Vector Extension (RVV) offering scalable data-level parallelism suitable for computationally intensive ML operations. However, the literature reveals a significant gap in comprehensive, production-ready implementations of vectorized ML kernels optimized specifically for RVV 1.0, along with limited empirical benchmarking across diverse kernel categories on actual vector processor implementations. This study addresses these gaps by developing RVV64\_Library, a comprehensive collection of RISC-V Vector-accelerated kernels targeting deep learning and scientific computing workloads. The research objectives include implementing optimized vectorized versions of fundamental ML primitives---encompassing matrix multiplication, convolution, transposed convolution, dense (fully connected) layers, batch normalization, activation functions (ReLU, Leaky ReLU, Softmax), max pooling, bias addition, tensor arithmetic, and ONNX-style indexing operations (Gather, Scatter, Non-Maximum Suppression)---while systematically evaluating performance across different LMUL (Length Multiplier) configurations (M1, M2, M4, M8). The methodology employs C++ implementations utilizing RVV 1.0 intrinsics, with functional correctness validated against ONNX golden references using QEMU emulation, and performance benchmarked on the Ara vector co-processor, an open-source implementation of a scalable RISC-V vector unit. The library architecture emphasizes modularity through reusable low-level vector wrappers for loads, stores, reductions, multiply-accumulate, and mask operations, enabling clean and maintainable kernel implementations. Python bindings via shared libraries further enhance accessibility for rapid experimentation. Performance evaluation on the Ara co-processor demonstrates substantial speedups ranging from 4$\times$ to over 70$\times$ compared to scalar baseline implementations. Notably, matrix multiplication achieves up to 70.27$\times$ improvement using unrolled vectorization strategies, while activation and normalization kernels such as Leaky ReLU, batch normalization, and max pooling achieve speedups between 20$\times$ and 36$\times$. Compute-intensive linear operators and pointwise arithmetic kernels consistently demonstrate significant acceleration across all tested configurations. The library's practical applicability is validated through complete end-to-end inference implementations of LeNet-5 for digit classification and Tiny-YOLOv2 for object detection, demonstrating seamless integration of vectorized kernels into real neural network pipelines. This work establishes that the RISC-V Vector Extension, when properly optimized, provides a viable, high-performance, and energy-efficient alternative for accelerating ML inference on resource-constrained embedded platforms.

\vspace{1em}
\noindent\textbf{Keywords:} RISC-V Vector Extension (RVV), Machine Learning Kernel Acceleration, Vector Co-processor, Deep Learning Inference, High-Performance Embedded Computing

\newpage