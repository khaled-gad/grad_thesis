% !TEX root = ../Thesis.tex
\section{The Developed Open Source Library}

This section describes the architecture of the open-source RaiVeX library, developed to provide accessible interfaces to RISC-V Vector Extension (RVV) accelerated neural network kernels. The library bridges the gap between low-level RVV intrinsics and high-level programming languages, enabling researchers and developers to leverage hardware acceleration without requiring expertise in assembly programming.

\subsection{Repository Structure}

The RaiVeX repository is organized as follows:

\begin{verbatim}
RaiVeX/
+-- kernels/
|   +-- batch_norm/
|   +-- bias_add/
|   +-- conv/
|   +-- conv_transpose/
|   +-- dense/
|   +-- gather/
|   +-- gather_elements/
|   +-- leaky_relu/
|   +-- matmul/
|   +-- maxpool/
|   +-- nms/
|   +-- relu/
|   +-- scatter_elements/
|   +-- softmax/
|   +-- tensor_add/
+-- lib/
|   +-- rvv_defs.h
|   +-- rvv_arithmetic.h
|   +-- ...
+-- libso/
+-- models/
|   +-- lenet-5/
|   +-- tiny-yolov2/
+-- pyv/
|   +-- kernels.py
|   +-- backend.py
|   +-- ...
+-- scripts/
+-- README.md
+-- requirements.txt
\end{verbatim}

\subsection{Explanation of Repository Contents}

The repository is structured to support the development, testing, and deployment of RVV-accelerated kernels. The \texttt{kernels/} directory contains individual kernel implementations, each organized in its own subdirectory with source code, test scripts, and build files. The \texttt{lib/} directory provides low-level RVV vector APIs implemented as C++ headers, wrapping RVV instructions into reusable building blocks such as vector loads/stores, mask operations, reductions, and multiply-accumulate operations. The \texttt{libso/} directory contains shared object files for building shared libraries. The \texttt{models/} directory includes complete neural network implementations using the kernels, such as LeNet-5 and Tiny-YOLOv2. The \texttt{pyv/} directory provides Python bindings for the kernels, enabling high-level usage. The \texttt{scripts/} directory contains utility scripts for testing and benchmarking.

The library implements kernels categorized according to the six computational patterns introduced in Section 3.1 of the Methodology chapter:

\begin{enumerate}
    \item \textbf{Compute-intensive FMA Operations}: Matrix multiplication (\texttt{kernels/matmul/}) and dense (fully connected layer, \texttt{kernels/dense/}).
    \item \textbf{Sliding-window kernels}: Convolution (\texttt{kernels/conv/}), transposed convolution \newline(\texttt{kernels/conv\_transpose/}), and max pooling (\texttt{kernels/maxpool/}).
    \item \textbf{Pointwise activations and elementwise arithmetic}: ReLU (\texttt{kernels/relu/}), Leaky ReLU (\texttt{kernels/leaky\_relu/}), tensor addition (\texttt{kernels/tensor\_add/}), and bias addition (\texttt{kernels/bias\_add/}).
    \item \textbf{Tensor indexing and data movement}: Gather (\texttt{kernels/gather/}), gather elements (\texttt{kernels/gather\_elements/}), and scatter elements (\texttt{kernels/scatter\_elements/}).
    \item \textbf{Statistical and normalization layers}: Batch normalization (\texttt{kernels/batch\_norm/}), and Softmax (\texttt{kernels/softmax/}).
    \item \textbf{Postprocessing (NMS)}: Non-maximum suppression (\texttt{kernels/nms/}).
\end{enumerate}

\subsection{Performance Results}

Performance measurements of the best performing RVV kernels are documented in the RaiVeX/BENCHMARKS.md
file, with detailed results available within the kernel-specific subdirectories.

\begin{itemize}
    \item Benchmarks and execution time comparisons for each kernel
          are located at \newline
          \texttt{RaiVeX/kernels/<kernel\_name>/benchmarks.md}
    \item Relative speedups against scalar baselines and other relevant metrics
          are summarized in the main \texttt{BENCHMARKS.md} file
\end{itemize}

This organization allows readers to locate and verify all performance results
directly from the repository.

\subsection{Wrappers Overview}

The library employs two types of wrappers to abstract RVV intrinsics for different levels of usage.

\subsubsection{Intrinsic Wrappers}

Intrinsic wrappers are implemented in C/C++ functions that abstract RVV intrinsics to improve readability and maintainability. These wrappers are defined in the \texttt{lib/} directory, such as \texttt{lib/rvv\_defs.hpp} and \texttt{lib/rvv\_arithmetic.hpp}. For example, a wrapper for vector addition might be implemented as:

\begin{lstlisting}[language=C++]
inline vfloat32m1_t VECTOR_ADD_VV(vfloat32m1_t a, vfloat32m1_t b, size_t vl) {
    return __riscv_vfadd_vv_f32m1(a, b, vl);
}
\end{lstlisting}

This abstraction allows kernel implementations in \texttt{kernels/} to use high-level function calls instead of direct intrinsics, enhancing code portability and maintainability.

\subsubsection{Python Wrappers}

Python bindings are provided in the \texttt{pyv/} directory, enabling higher-level and real-world usage of the kernels. The bindings use ctypes to interface with compiled C shared libraries, allowing seamless integration with Python machine learning workflows. For instance, the ReLU kernel can be invoked from Python as shown in \texttt{models/lenet-5/py/main.py}:

\begin{lstlisting}[language=Python]
from pyv.kernels import relu
output = relu(input_tensor, variant="M8")
\end{lstlisting}

This approach provides NumPy compatibility, automatic memory management, and variant selection for performance tuning.

% \subsection{Results Verification}

% The correctness of the RVV kernel implementations is assessed by referencing
% the benchmark results documented in the repository. The main summary of
% benchmarks is provided in
% \href{https://github.com/OmarAly03/RaiVeX/blob/main/BENCHMARKS.md}{BENCHMARKS.md}.

% For each kernel, verification results and performance summaries are located in
% the respective kernel directories:

% \begin{itemize}
%     \item \texttt{RaiVeX/kernels/<kernel\_name>/benchmarks.md} â€“ contains
%           the detailed results for the corresponding kernel
%     \item Representative kernels include GEMM, convolution, and other
%           vectorized operations
% \end{itemize}

% This approach provides a central reference for all correctness and benchmark
% results without requiring additional test frameworks.

\subsection{Wrapper Interfaces}

The Python wrappers provide consistent, NumPy-compatible interfaces for all kernels. Each wrapper handles:

\begin{itemize}
    \item Input tensor validation and shape checking
    \item Automatic memory allocation for output tensors
    \item Variant selection based on input characteristics
    \item Error handling and informative error messages
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Kernel Variants and Implementation Availability}
\label{tab:kernel_variants}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Kernel} & \textbf{Scalar} & \textbf{M1} & \textbf{M2} & \textbf{M4} & \textbf{M8} & \textbf{Tiled} \\
\hline
ReLU & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & -- \\
\hline
Leaky ReLU & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\hline
MatMul & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & -- \\
\hline
Tensor Add & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & -- \\
\hline
Batch Norm & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\hline
Bias Add & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & -- \\
\hline
Conv2D & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & 3x3 Specialized \\
\hline
Conv2D Transpose & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & 3x3 Specialized \\
\hline
Dense (FC) & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & -- \\
\hline
MaxPool & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\hline
Softmax & \checkmark & -- & -- & -- & -- & -- \\
\hline
\end{tabular}
\end{table}
\newpage
\subsubsection{Example Function Signature}

As an example, the ReLU activation function signature in \texttt{pyv/kernels.py} is:

\begin{lstlisting}[language=Python]
def relu(x: np.ndarray, variant: str = "M8") -> np.ndarray:
    """
    Apply ReLU activation element-wise to input tensor.
    
    Args:
        x: Input tensor of any shape, dtype float32
        variant: Implementation variant ("scalar", "M1", "M2", "M4", "M8")
    
    Returns:
        Output tensor with same shape as input, ReLU applied element-wise
    """
\end{lstlisting}

This signature demonstrates the consistent interface design across all kernel wrappers, with optional variant selection for performance tuning.\\[2pt]

This chapter has described the architecture and implementation of the RaiVeX open-source library. It detailed the repository organization, the categorization of kernels by computational pattern, and the design of intrinsic and Python wrappers that abstract RVV intrinsics into reusable interfaces. By coupling low-level vector primitives with NumPy-compatible Python bindings and clearly documented benchmarks, the library enables practical adoption of RVV-accelerated kernels in real-world machine learning workloads and provides a solid foundation for future extensions on emerging RISC-V vector platforms.