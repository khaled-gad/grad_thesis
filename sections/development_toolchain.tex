% !TEX root = ../Thesis.tex
\subsection{Development Toolchain}

\subsubsection{RISC-V GNU Toolchain}

The RISC-V GNU Toolchain provides the foundational software infrastructure required to develop, compile, link, and debug programs targeting the RISC-V instruction set architecture. In this project, the toolchain serves as the primary means of compiling and validating RISC-V Vector Extension (RVV)â€“based kernel implementations prior to functional verification and RTL-level evaluation.

The toolchain was selected due to its mature support for the RISC-V ecosystem, active community maintenance, and integration of RVV 1.0 intrinsics within the GNU Compiler Collection (GCC). This enables the development of vectorized kernels using high-level C/C++ code while maintaining explicit control over generated vector instructions.

\paragraph{Toolchain Components}

\textbf{Compiler (\texttt{riscv64-unknown-linux-gnu-g++}):}  
The GCC compiler translates C/C++ source code into RISC-V assembly and object code. In this work, vectorization is primarily achieved through explicit RVV intrinsics rather than relying on automatic vectorization. This approach ensures deterministic mapping between source-level kernel implementations and the generated vector instructions, which is essential for functional verification and architectural analysis.

The following architecture-specific compilation flags are used throughout the project:
\begin{itemize}
    \item \texttt{-march=rv64gcv}: Enables the RV64GCV ISA, including the RISC-V Vector Extension
    \item \texttt{-mabi=lp64d}: Specifies the 64-bit ABI with double-precision floating-point support
    \item \texttt{-O2}, \texttt{-O3}: Enable standard compiler optimizations without altering the explicit vectorization strategy
\end{itemize}

\textbf{Assembler (\texttt{riscv64-unknown-linux-gnu-as}):}  
Converts RISC-V assembly code into object files, supporting both scalar and RVV instruction encodings. It is used indirectly through the compiler toolchain and for inspecting compiler-generated assembly during kernel analysis.

\textbf{Linker (\texttt{riscv64-unknown-linux-gnu-ld}):}  
Links object files and resolves external symbols to produce final executable binaries suitable for execution under emulation or simulation.

\textbf{Debugger (\texttt{riscv64-unknown-linux-gnu-gdb}):}  
Provides source-level debugging capabilities for RISC-V binaries executed under QEMU, including inspection of scalar and vector registers. This functionality is essential for validating intermediate kernel states during development.

\textbf{Binary Utilities:}  
Tools such as \texttt{objdump}, \texttt{readelf}, and \texttt{nm} are used to inspect binaries, verify RVV instruction generation, and analyze symbol information.

\textbf{Runtime Libraries:}  
The GNU C Library (\texttt{glibc}) is used for Linux-based execution under QEMU, providing standard runtime support required for user-mode and system-mode emulation. Bare-metal runtime support is not a primary focus of this work.

Collectively, the RISC-V GNU Toolchain constitutes the software development layer of the project, enabling portable kernel implementation and serving as the entry point for subsequent functional and RTL-level verification.

% ------------------------------------------------------------

\subsubsection{QEMU Emulator}

QEMU (Quick Emulator) is employed as the primary functional execution environment for RISC-V binaries in this project. QEMU provides architecturally correct emulation of the RISC-V ISA, including the RISC-V Vector Extension, and is used exclusively for functional correctness validation and software-level testing. Performance results reported in this thesis are derived from RTL simulation rather than QEMU execution.

QEMU was selected over alternative simulators due to its system-level completeness, integration with standard debugging tools, and practical support for executing complex software stacks.

\paragraph{Rationale for Using QEMU}

While Spike is the official RISC-V ISA simulator, QEMU offers several advantages aligned with the goals of this project. QEMU supports full Linux execution environments, enabling realistic testing of vectorized kernels in the presence of system calls, memory management, and runtime libraries. Additionally, QEMU integrates seamlessly with GDB, allowing interactive debugging and inspection of vector register state.

QEMU employs dynamic binary translation, which significantly reduces simulation time compared to purely interpretive simulators. This improvement in execution speed enhances development productivity when validating functional correctness across a large number of test cases and kernel configurations. Furthermore, modern QEMU versions provide architecturally accurate support for RVV 1.0 instructions, enabling reliable ISA-level validation.

\paragraph{QEMU User-Mode Emulation}

QEMU user-mode emulation is the primary execution mode used throughout this project. In this mode, RISC-V Linux binaries are executed directly on the host system without emulating a full hardware platform:

\begin{lstlisting}[language=bash]
qemu-riscv64 -cpu rv64,v=true,vlen=256 ./my_program
\end{lstlisting}

User-mode emulation offers fast startup times, direct access to the host file system, and straightforward integration with development tools. Importantly, it allows the vector length (\texttt{VLEN}) to be configured at runtime, enabling validation of Vector Length Agnostic (VLA) kernel behavior across multiple vector configurations.

\paragraph{QEMU System-Mode Emulation}

In addition to user-mode execution, QEMU system-mode emulation is employed during the development of Python wrappers for the kernel library. System-mode emulation provides a complete virtualized RISC-V machine, including bootloader, kernel, and virtual devices, enabling end-to-end testing of library integration within a full operating system environment.

This mode is particularly valuable for validating language bindings, dynamic linking behavior, and interaction between Python-based tooling and the underlying RISC-V kernel implementations. While system-mode execution incurs higher simulation overhead, it provides a realistic software stack that closely mirrors deployment scenarios.

\paragraph{Role in the Verification Workflow}

Within the overall methodology, QEMU serves as the functional validation layer. Kernel outputs produced under QEMU are compared against ONNX-based golden references to ensure correctness and portability. Once functional correctness is established, performance and microarchitectural behavior are evaluated using cycle-accurate RTL simulation on the Vicuna and Ara cores.


%%%%%%%%%%%%%%%%% ONNX SECTION %%%%%%%%%%%%%%%%%

\subsubsection{ONNX: Open Neural Network Exchange}


The Open Neural Network Exchange (ONNX) is an open standard designed to represent machine learning and deep learning models in a framework-independent manner. It was originally established through a collaboration between Facebook and Microsoft with the objective of improving interoperability across machine learning frameworks, tools, and hardware platforms.

ONNX defines a common intermediate representation for machine learning models, enabling them to be exported from one framework and executed or analyzed in another without modification. This representation is based on a computational graph abstraction, where nodes correspond to standardized operators and edges represent the flow of multi-dimensional tensors between operators.

The adoption of ONNX has been widely supported by both academia and industry, and it is now integrated into many popular machine learning frameworks and deployment toolchains. Its design emphasizes portability, reproducibility, and long-term maintainability, making it particularly suitable for systems-level research and hardware-oriented optimization efforts.

In the context of this project, ONNX serves as a unifying abstraction layer between high-level machine learning models and low-level, architecture-specific kernel implementations targeting the RISC-V Vector Extension.

\paragraph{Components of ONNX and Their Role in the Project}

\subparagraph{ONNX Model Components}

An ONNX model represents a machine learning computation as a directed computational graph, where nodes correspond to standardized operators and edges represent the flow of multi-dimensional tensor data between operators. This graph-based representation explicitly defines model inputs, outputs, and intermediate computations, providing a clear and structured description of the overall computation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\textwidth]{figures/ONNX-GRAPH-EX.png}
    \caption{ONNX Model Computational Graph example illustrating nodes (operators) and edges (tensor data flow).}
    \label{fig:onnx_graph}
\end{figure}

ONNX operators are drawn from a predefined and versioned operator set, with each operator having deterministic and well-specified mathematical semantics. In addition, ONNX models explicitly define tensor data types, shapes, and constant parameters, enabling consistent interpretation of computations across different software and hardware platforms. This standardized structure allows ONNX models to serve as precise and reproducible representations of intended kernel behavior.

\subparagraph{Role of ONNX in the Project}

In this project, ONNX models are used as golden references for the functional validation of machine learning kernels implemented using the RISC-V Vector Extension. The ONNX representation mirrors the functionality of the developed kernels by explicitly defining the same inputs, outputs, and computational operations, independent of any specific hardware implementation.

The RVV-based kernel outputs are compared directly against the corresponding outputs generated from ONNX models executed using a validated ONNX runtime. Since ONNX provides a standardized and hardware-agnostic format with deterministic operator behavior, it serves as a reliable baseline for correctness verification. This approach ensures that discrepancies in output can be attributed to kernel implementation issues rather than ambiguities in operator definitions or execution semantics.

By adopting ONNX as the functional reference, the project achieves reproducible and framework-independent validation, strengthening confidence that the optimized vectorized kernels preserve the intended computational behavior while improving performance.

%%%%%%%%%%%%%%%%%% RTL CORES SECTION %%%%%%%%%%%%%%%%%

\subsubsection{RTL Cores and Hardware Simulation}

Hardware simulation using Register Transfer Level (RTL) cores plays a critical role in the development and evaluation of vectorized machine learning kernels. While functional simulators are sufficient for validating instruction set compliance, they lack the temporal accuracy required to capture microarchitectural effects such as pipeline behavior, memory contention, and vector execution overheads. For this reason, this project employs cycle-accurate RTL cores to provide a realistic evaluation environment for RISC-V Vector Extension (RVV)-based kernel development.

Two RTL vector processors are used in this work: the \textit{Vicuna} vector coprocessor and the \textit{Ara} vector processor. Together, they represent complementary points in the RISC-V vector design space and provide a robust simulation foundation for functional and performance-oriented analysis.

\paragraph{Vicuna RISC-V Vector Coprocessor}

Vicuna is a 32-bit RISC-V vector coprocessor designed with a primary emphasis on timing predictability and deterministic execution. It targets the \texttt{Zve32x} subset of the RVV 1.0 specification, making it well-suited for embedded and edge-class workloads that rely on integer and fixed-point vector operations, such as quantized neural networks.

The architecture prioritizes worst-case execution time (WCET) analyzability by avoiding microarchitectural features that introduce timing variability, such as out-of-order execution or banked register files. As a result, Vicuna provides a cycle-accurate and bit-accurate simulation environment in which vector instruction latency is a deterministic function of vector length and hardware configuration. In this project, Vicuna serves as a baseline RTL platform for evaluating vector execution behavior in predictable embedded-class systems.

\paragraph{Ara Vector Processor}

Ara is a 64-bit, application-class RISC-V vector processor designed to maximize throughput and floating-point utilization for high-performance computing and machine learning workloads. It implements the full RVV 1.0 specification, supporting a wide range of data types, including IEEE-754 single- and double-precision floating-point formats.

Ara employs a lane-based vector architecture with scalable parallelism and supports advanced features such as vector chaining, masked execution, and high-bandwidth vector memory operations. Its tight integration with a scalar host core enables efficient amortization of instruction overhead across long vector sequences, making it particularly well-suited for compute-intensive kernels such as matrix multiplication, convolution, and activation functions.

In this project, Ara provides a high-fidelity RTL simulation environment for evaluating the performance and correctness of vectorized machine learning kernels under realistic architectural constraints.



\subparagraph{\textbf{A detailed architectural analysis, execution model discussion, and performance evaluation of both the Vicuna and Ara cores are presented in Section~4 of this thesis.}}
