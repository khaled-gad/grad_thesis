% !TEX root = ../Thesis.tex
\subsection{Development Toolchain}

\subsubsection{RISC-V GNU Toolchain}

The RISC-V GNU Toolchain provides the foundational components necessary to build, compile, link, and debug code for the RISC-V ISA. This toolchain was selected for its active community support, regular updates, and comprehensive support for both native and emulated environments, including full support for the vector extension.

\paragraph{Toolchain Components}

\textbf{Compiler (\texttt{riscv64-unknown-linux-gnu-gcc}):} Translates C/C++ source code into RISC-V assembly and object code. The compiler includes full support for RISC-V vector intrinsics, allowing developers to write vectorized code using high-level C/C++ functions that map directly to vector instructions. Architecture-specific flags enable vector extension support, including:
\begin{itemize}
    \item \texttt{-march=rv64gcv}: Enables the full RV64GCV ISA including vector extensions
    \item \texttt{-O2}, \texttt{-O3}: Optimization levels that leverage vector instructions
    \item \texttt{-mabi=lp64d}: Specifies the 64-bit ABI with double-precision floating-point
\end{itemize}

\textbf{Assembler (\texttt{riscv64-unknown-linux-gnu-as}):} Converts assembly code into object files, supporting both standard RISC-V instructions and vector extension opcodes.

\textbf{Linker (\texttt{riscv64-unknown-linux-gnu-ld}):} Links multiple object files and resolves external references to produce final executable binaries.

\textbf{Debugger (\texttt{riscv64-unknown-linux-gnu-gdb}):} Enables source-level debugging and inspection of RISC-V binaries running under emulation, with support for examining vector register contents.

\textbf{Binary Utilities:} Tools like \texttt{objdump}, \texttt{readelf}, and \texttt{nm} for inspecting compiled binaries, verifying instruction encoding, and analyzing symbol tables.

\textbf{Runtime Libraries:} Standard runtime support including newlib and glibc, providing C standard library functionality for both bare-metal and Linux environments.

\subsubsection{QEMU Emulator}

QEMU (Quick Emulator) serves as our primary execution environment for RISC-V code. As a full system emulator, QEMU provides comprehensive support for user-mode and full-system emulation, including the RISC-V vector extension in recent versions.

\paragraph{Why QEMU Over Spike}

While Spike is the official RISC-V ISA simulator from the RISC-V Foundation, we chose QEMU for several key advantages:

\textbf{System Completeness:} QEMU emulates entire Linux-based systems, not just the ISA, enabling realistic testing of our implementations including system calls, memory management, and I/O operations.

\textbf{Debugging Integration:} QEMU integrates seamlessly with GDB, allowing source-level debugging with breakpoints, watchpoints, and inspection of both scalar and vector registers.

\textbf{Performance:} QEMU uses dynamic binary translation, providing faster execution than Spike's interpretive approach, which is crucial when running complex neural network workloads.

\textbf{Vector Extension Support:} Modern QEMU versions include comprehensive RVV support, accurately emulating vector instructions including the latest intrinsics.

\textbf{Community and Documentation:} QEMU benefits from wider usage, more extensive documentation, and more active development than Spike, with a large community providing support and bug fixes.

\paragraph{QEMU User Mode Execution}

Our typical workflow uses QEMU in user-mode emulation, where RISC-V Linux binaries are executed directly on the host system:

\begin{lstlisting}[language=bash]
qemu-riscv64 -cpu rv64,v=true,vlen=256 ./my_program
\end{lstlisting}

This approach provides:
\begin{itemize}
    \item Fast startup times
    \item Direct access to host file system
    \item Straightforward integration with development tools
    \item Configurable vector length (VLEN) for testing portability
\end{itemize}


%%%%%%%%%%%%%%%%% ONNX SECTION %%%%%%%%%%%%%%%%%

\subsubsection{ONNX: Open Neural Network Exchange}


The Open Neural Network Exchange (ONNX) is an open standard designed to represent machine learning and deep learning models in a framework-independent manner. It was originally established through a collaboration between Facebook and Microsoft with the objective of improving interoperability across machine learning frameworks, tools, and hardware platforms.

ONNX defines a common intermediate representation for machine learning models, enabling them to be exported from one framework and executed or analyzed in another without modification. This representation is based on a computational graph abstraction, where nodes correspond to standardized operators and edges represent the flow of multi-dimensional tensors between operators.

The adoption of ONNX has been widely supported by both academia and industry, and it is now integrated into many popular machine learning frameworks and deployment toolchains. Its design emphasizes portability, reproducibility, and long-term maintainability, making it particularly suitable for systems-level research and hardware-oriented optimization efforts.

In the context of this project, ONNX serves as a unifying abstraction layer between high-level machine learning models and low-level, architecture-specific kernel implementations targeting the RISC-V Vector Extension.

\paragraph{Components of ONNX and Their Role in the Project}

\subparagraph{ONNX Model Components}

An ONNX model represents a machine learning computation as a directed computational graph, where nodes correspond to standardized operators and edges represent the flow of multi-dimensional tensor data between operators. This graph-based representation explicitly defines model inputs, outputs, and intermediate computations, providing a clear and structured description of the overall computation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\textwidth]{figures/ONNX-GRAPH-EX.png}
    \caption{ONNX Model Computational Graph example illustrating nodes (operators) and edges (tensor data flow).}
    \label{fig:onnx_graph}
\end{figure}

ONNX operators are drawn from a predefined and versioned operator set, with each operator having deterministic and well-specified mathematical semantics. In addition, ONNX models explicitly define tensor data types, shapes, and constant parameters, enabling consistent interpretation of computations across different software and hardware platforms. This standardized structure allows ONNX models to serve as precise and reproducible representations of intended kernel behavior.

\subparagraph{Role of ONNX in the Project}

In this project, ONNX models are used as golden references for the functional validation of machine learning kernels implemented using the RISC-V Vector Extension. The ONNX representation mirrors the functionality of the developed kernels by explicitly defining the same inputs, outputs, and computational operations, independent of any specific hardware implementation.

The RVV-based kernel outputs are compared directly against the corresponding outputs generated from ONNX models executed using a validated ONNX runtime. Since ONNX provides a standardized and hardware-agnostic format with deterministic operator behavior, it serves as a reliable baseline for correctness verification. This approach ensures that discrepancies in output can be attributed to kernel implementation issues rather than ambiguities in operator definitions or execution semantics.

By adopting ONNX as the functional reference, the project achieves reproducible and framework-independent validation, strengthening confidence that the optimized vectorized kernels preserve the intended computational behavior while improving performance.

%%%%%%%%%%%%%%%%%% RTL CORES SECTION %%%%%%%%%%%%%%%%%

\subsubsection{RTL Cores and Hardware Simulation}

Hardware simulation using Register Transfer Level (RTL) cores plays a critical role in the development and evaluation of vectorized machine learning kernels. While functional simulators are sufficient for validating instruction set compliance, they lack the temporal accuracy required to capture microarchitectural effects such as pipeline behavior, memory contention, and vector execution overheads. For this reason, this project employs cycle-accurate RTL cores to provide a realistic evaluation environment for RISC-V Vector Extension (RVV)-based kernel development.

Two RTL vector processors are used in this work: the \textit{Vicuna} vector coprocessor and the \textit{Ara} vector processor. Together, they represent complementary points in the RISC-V vector design space and provide a robust simulation foundation for functional and performance-oriented analysis.

\paragraph{Vicuna RISC-V Vector Coprocessor}

Vicuna is a 32-bit RISC-V vector coprocessor designed with a primary emphasis on timing predictability and deterministic execution. It targets the \texttt{Zve32x} subset of the RVV 1.0 specification, making it well-suited for embedded and edge-class workloads that rely on integer and fixed-point vector operations, such as quantized neural networks.

The architecture prioritizes worst-case execution time (WCET) analyzability by avoiding microarchitectural features that introduce timing variability, such as out-of-order execution or banked register files. As a result, Vicuna provides a cycle-accurate and bit-accurate simulation environment in which vector instruction latency is a deterministic function of vector length and hardware configuration. In this project, Vicuna serves as a baseline RTL platform for evaluating vector execution behavior in predictable embedded-class systems.

\paragraph{Ara Vector Processor}

Ara is a 64-bit, application-class RISC-V vector processor designed to maximize throughput and floating-point utilization for high-performance computing and machine learning workloads. It implements the full RVV 1.0 specification, supporting a wide range of data types, including IEEE-754 single- and double-precision floating-point formats.

Ara employs a lane-based vector architecture with scalable parallelism and supports advanced features such as vector chaining, masked execution, and high-bandwidth vector memory operations. Its tight integration with a scalar host core enables efficient amortization of instruction overhead across long vector sequences, making it particularly well-suited for compute-intensive kernels such as matrix multiplication, convolution, and activation functions.

In this project, Ara provides a high-fidelity RTL simulation environment for evaluating the performance and correctness of vectorized machine learning kernels under realistic architectural constraints.



\subparagraph{\textbf{A detailed architectural analysis, execution model discussion, and performance evaluation of both the Vicuna and Ara cores are presented in Section~4 of this thesis.}}
